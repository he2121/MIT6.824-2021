---
title: "lecture-note-03"
date: 2021-12-18T14:30:00+08:00
tags: ["6.824", "翻译", "note"]
---

# 6.824 2020 Lecture 3: GFS

[笔记原地址](https://pdos.csail.mit.edu/6.824/notes/l-gfs.txt)

[论文](https://pdos.csail.mit.edu/6.824/papers/gfs.pdf)

## 课程背景

GFS(The Google FIle System) 谷歌文件系统 By Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung

SOSP 2003

### 为什么我们要研究这篇论文？

1. 分布式存储是一个关键的抽象概念

   - 这些接口/语义应该是什么样子
   - 其内部是如何运作的
2. GFS 涉及到很多 6.824 的主题

   - 并行性能
   - 容错
   - 复制
   - 一致性
3. 优秀的系统论文：从应用程序到网络的所有细节
3. 成功的现实设计

### 为什么分布式存储难？

- 高性能：需要在许多服务器上分片
- 许多服务器：总是会有故障
- 容错：复制
- 复制：可能会导致数据不一致
- 好的数据一致性：低性能

### 我们希望有怎样的一致性？

- 理想的模型：与单个服务器的行为一致

- 服务使用磁盘存储

- 服务器一次执行一个客户操作（即使是并发的）。

- 读取的内容反映了之前的写入内容，即使服务器崩溃并重新启动

因此：假定 C1 和 C2 同时写，在写完后。C3，C4 读，他们能看到什么？

​	C1：Wx1

​	C2：Wx2

​	C3：			Rx

​	C4:						Rx

答案：读到 1 和 2 即可。但是两个必须看到是同一个值。这就是强一致性模型。但是一台机器容错性太差。

### 容错的需要复制，使强一致性变得很棘手

1. 一个简单但是有问题的复制方案

   - 有两台副本服务器，S1 和 S2
   - client 并行向两台服务器写
   - client 在任意一台上读
2. 在我们的例子中，C1 和 C2 的写命令可能以不同的顺序到达两个副本
   - 如果 C3 读 S1， 可能会看到 x = 1，而当 C4 读 S2，可能会看到 x = 2
   - 或者 S1 收到了第一个写，在第二个写到来前崩溃了。
     - 上述两者都导致了强烈的不一致性
3. 更好的一致性通常需要通信来确保复制体之间保持同步 -- 可能很慢。在性能和一致性之间可能有很多折衷办法，我们今天会看到一个

## GFS 论文内容

### 论文背景

1. 许多谷歌服务需要一个大并且快速统一的存储系统
   - MapReduce，crawler/indexer, log storage/analysis, Youtube

2. 全局的（单一的数据中心）：任何 client 都可以读取任何文件，允许应用程序之间共享数据

3. 自动将每个文件分片到许多服务器/磁盘上

   - 为了并发性能
   - 为了增加可用空间

4. 自动从故障中复原

5. 每次部署一个数据中心

6. 仅谷歌内部应用和用户使用

7. 针对的是对巨大文件的顺序访问：读取或追加

   - 不是针对小文件的低时延数据库
#### 这篇 2003 年论文有什么新的内容？怎样被 SOSP 论文接受的

而不是其分布式、分片、容错的基本思想比较新，而是因为

- 巨大的规模
- 在工业中使用，有真实的经验。
- 成功地使用在弱的一致性情形下。
- 成功地使用在单一的 master 情形下

### 总体结构

- 客户端（使用 库、RPC 接入-- 实现细节 UNIX 文件系统不可见）

- 每个文件被分割成独立的64MB的小块

- 块服务器(chunk server)，每个块复制在 3 个块服务器上
- 每个文件的块都分散在块服务器上
  - 用于并行读/写（如MapReduce），为了存储巨大的文件
- 单个 master
- 工作分工：master 负责命名与数据映射，chunk server 负责数据。

#### Master 状态

- 在 RAM 中（为了速度，必须是小规模的）
  - 文件名 --> 此文件所拥有的块 ID 的数组 （不是易丢失的）
  - 块 ID  ---> 版本（不是易丢失的），一串对应的 chunkserver 地址（易丢失的），主（primary） chunkserver （易丢失的），租期（易丢失的）
- 在磁盘中
  - log
  - checkpoint

##### 为什么是 log 和 checkpoint？

##### 为什么块（chunk）这么大（64M）

####  客户端 C 读一个文件的步骤？

1. C 把文件名和读取偏移量发送到 Master M（如果没有被缓存）
2. M 通过文件名与读取偏移量找到对应 chunk ID
3. M 回应那些拥有最新版本 chunkserver 给 C
4. C 把接收到的 chunkserver 缓存下来
5. C 向最近的 chunkserver 发送请求，块 ID，偏移量
6. chunk server 从磁盘上的 chunk 文件中读取信息，返回

##### master 如何知道哪些 chunkservers 拥有某个特定的chunk？

#### 当C想做一个 "记录追加 "时，有哪些步骤？

论文图 2

1. C 向 M询问文件的最后一个 chunk。
2. 如果 M 看到最后 chunk 没有 primary（或租约过期）。
   1. 如果没有最新版本的 chunkservers ，则返回错误
   2. 从那些有最新版本的 chunkservers 中挑选主要的 P 和次要的。
   3. 递增版本，写入磁盘上的日志
   4. 告诉 P 和次要他们身份，以及新的版本
   5. replia 将新的版本，写到磁盘上
3. M 告诉 C 主要和次要的 chunkserver
4. C 向所有 chunkserver 发送数据（只是临时的......），等待
5. C 告诉 P 追加
6. P 检查租约是否过期，并且块是否存在足够的空间。
7. P 选择一个偏移量（在块的末端）
8. P 写入块文件（一个Linux文件）
9. P 告诉每个次级文件的偏移量，告诉其追加到大块文件中
10. P 等待所有次级的回复，或超时。次级可以回答 "错误"，例如，没有磁盘空间了。
11. P 告诉 C "OK "或 "错误"
12. 如果出错，C会从头再试一次

---

暂时不更了，发现 lecture 3，4 收益不高
